**ğŸš€ CSV to MySQL Pipeline with Airflow and Docker**
This project implements a scalable data pipeline that loads one or more CSV files into a MySQL database
using Apache Airflow. It uses csvsql to generate table schemas automatically from CSV headers, supports
parallel processing, and is fully containerized with Docker.

**âœ¨ Features**
âœ… Accepts multiple CSV files

âœ… Auto-generates MySQL schema with csvsql

âœ… Loads data into MySQL

âœ… Parallel processing using Airflow

âœ… Fully containerized with Docker and Docker Compose

**ğŸ›  Technologies Used**
Apache Airflow â€“ Workflow orchestration

MySQL â€“ Relational database

csvsql (from csvkit) â€“ Schema generation

Docker â€“ Containerized runtime environment

**âš™ï¸ Configuration**
The only required configuration is setting the CSV delimiter in the DAG file.

In dags/load_to_database.py, set:
delimiter = ','
You can change this to ';', '\t', or any other delimiter that matches your CSV files.

**ğŸ“‚ Project Structure**
.
â”œâ”€â”€ airflow/                      # Airflow environment setup
â”‚   â”œâ”€â”€ Dockerfile                # Custom Airflow Docker image
â”‚   â””â”€â”€ airflow.cfg              # Airflow configuration
â”‚
â”œâ”€â”€ dags/                         # DAG files
â”‚   â””â”€â”€ load_to_database.py      # Main DAG that processes CSV files
â”‚
â”œâ”€â”€ data/                         # Folder to place your CSV files
â”‚   â””â”€â”€ your_files.csv
â”‚
â”œâ”€â”€ docker-compose.yml           
â””â”€â”€ README.md

**â–¶ï¸ Usage**
Add your CSV files
Place one or more .csv files inside the data/ folder.

Set your delimiter
Open dags/load_to_database.py and adjust the delimiter variable if needed.

Build and run the pipeline
In the root directory, run:

docker-compose up --build

Access Airflow UI
Navigate to: http://localhost:8082

Trigger the DAG
Run the load_to_database DAG to start processing files.


**ğŸ‘¤ Made By**
Abdelghafour AIT ADDI
aitaddiabdelghafour@gmail.com
